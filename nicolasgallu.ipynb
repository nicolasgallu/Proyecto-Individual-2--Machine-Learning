{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROYECTO INDIVIDUAL NUMERO 2\n",
    "##### Nicolas Gallucci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importacion de librerias necesarias para la etapa de carga y preprocesamiento de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Importamos las bases de datos de entrenamiento y testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv (\"Datathon\\properties\\properties_colombia_train.csv\")\n",
    "test = pd.read_csv (\"Datathon\\properties\\properties_colombia_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Seteo de configuracion necesaria dentro de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Analisis exploratorio de los datos con Profile Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "ProfileReport(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Normalizacion de valores vacios cambiandolos por el valor de \"nan\" para lograr una tranformacion mas rapida y precisa de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.replace(r'^\\s*$', np.nan, regex=True,inplace=True)\n",
    "test.replace(r'^\\s*$', np.nan, regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Pequeño scraping para obtener el valor de la divisa colombiana en dolares para poder completar los valores que estan en dicha moneda dentro del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_8996\\675915861.py:23: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  tabla[\"peso_colombiano\"] = tabla[\"peso_colombiano\"].str.replace(\"$\",\"\")\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.capitalcolombia.com/index.php?sec=trm_precio_dolar_en_colombia&pag=ano&consulta=2020\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "precio = soup.find_all(\"table\",class_=\"table table-striped table-bordered col-10\")\n",
    "#busco dentro del url con ayuda de request y soup, el codigo html que me brinda la informacion del registro diario del año 2020 sobre el precio del dolar en Colombia.\n",
    "\n",
    "x = str(precio).split()\n",
    "x = re.findall(r'align=\"center\">\\S*[^0-9]',str(x).replace(\",\",\"\"))\n",
    "peso_colombiano = re.findall(r'[$]+\\d......',str(x))\n",
    "fecha = re.findall(r'[^$]\\d....2020',str(x))\n",
    "\n",
    "#Utilizo Regex para filtrar los datos que necesito y asi crear el dataframe.\n",
    "\n",
    "tabla = pd.DataFrame({\n",
    "        \"peso_colombiano\":peso_colombiano,\n",
    "        \"start_date\":fecha\n",
    "        })\n",
    "\n",
    "tabla['start_date'] = pd.to_datetime(tabla['start_date'], format=\"%d/%m/%Y\")\n",
    "tabla['start_date'].replace(\"%d/%m/%Y\",\"%Y/%m/%d\",inplace=True)\n",
    "tabla['start_date'] = tabla['start_date'].astype(str)\n",
    "\n",
    "tabla[\"peso_colombiano\"] = tabla[\"peso_colombiano\"].str.replace(\"$\",\"\")\n",
    "tabla['peso_colombiano'] = tabla['peso_colombiano'].astype(float)\n",
    "\n",
    "#Configuro la tabla para que las fechas me queden en el mismo orden que el dataset de entrenamiento, de esa manera, en los proximos pasos\n",
    "#podre concadenarlas y normalizar los precios dolarizados dentro del set de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Columnas a eliminar (explicado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=[\"Unnamed: 0\",\"end_date\",\"id\",\"ad_type\",\"l1\",\"l4\",\"l5\",\"l6\",\"surface_total\",\n",
    "                        \"surface_covered\",\"title\",\"description\",\"rooms\",\"bedrooms\",\"price_period\",\n",
    "                        \"operation_type\",\"created_on\",\"geometry\"],inplace=True)\n",
    "                        \n",
    "test.drop(columns=[\"Unnamed: 0\",\"end_date\",\"id\",\"ad_type\",\"l1\",\"l4\",\"l5\",\"l6\",\"surface_total\",\n",
    "                        \"surface_covered\",\"title\",\"description\",\"rooms\",\"bedrooms\",\"price_period\",\n",
    "                        \"operation_type\",\"created_on\",\"geometry\",\"currency\",\"start_date\"],inplace=True)\n",
    "\n",
    "\n",
    "# Las 3 razones mas importantes por las cuales elimine estas columnas a priori, fue porque la mayoria contaban con mas del 80% de valores faltantes (surface_total,rooms,bedrooms,l6,l5,l4...).\n",
    "# Otras eran columnas que solo ofrecian informacion muy variada y categorica que me iba a ser imposible modelar (description,title,...),\n",
    "# y el resto por ser columnas sin informacion relevante, o ser redundantes para el modelo (Unnamed: 0,id,ad_type,created_on...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Luego del analisis exploratorio y teniendo en mente que columnas voy a dropear y porque, vamos a completar aquellas columnas con valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COLUMNA DE PRECIOS\n",
    "\n",
    "train[\"currency\"].value_counts() \n",
    "#-- 8 valores en USD, son insignificantes en comparacion al dataset recibido de entrenamiento, \n",
    "# pero podria ser una buena practica si el dataset en un futuro incrementara, y asi los casos tambien de viviendas en dolares.\n",
    "\n",
    "\n",
    "train = pd.merge(train, tabla, on='start_date')\n",
    "train[\"price\"][train[\"currency\"] == \"USD\"] = train[\"price\"] * train[\"peso_colombiano\"] \n",
    "train.drop(columns=[\"peso_colombiano\",\"currency\",\"start_date\"],inplace=True)           \n",
    "#-- Normalizo a precio colombianos, combinando las tablas \"train\" y \"tabla\" y luego dropeando las columnas                                                                                        \n",
    "# \"peso_colombiano\" , \"currency\" (ya que esta ultima es redundante) y \"start_date\" que no me servira para para modelo de clasificacion.\n",
    "\n",
    "\n",
    "train[\"price\"].isna().sum() #--- 63 valores faltantes\n",
    "train[\"price\"][train[\"price\"].isna()] = train[\"price\"].mean() #-- Completo los faltantes con el promedio de todos los precios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COLUMNA DE BAÑOS\n",
    "\n",
    "promedios = []  \n",
    "\n",
    "for i in range(len(train.property_type.value_counts().index)):  \n",
    "    tipo = train.property_type.value_counts().index[i],train.bathrooms [train.property_type == train.property_type.value_counts().index[i]].mean()\n",
    "    promedios.append(tipo)\n",
    "#-- Con este loop consigo el promedio de baños por tipo de propiedad dentro de la lista \"promedios\", de esta manera podre continuar aquellas \n",
    "# propiedades con baños faltantes de forma mas precisa.\n",
    "\n",
    "for i in range (len(promedios)): #-- En cada instancia completo la columna de baños faltantes con su respectivo promedio.\n",
    "    train.bathrooms[(train.bathrooms.isna()) & (train.property_type == promedios[i][0])] = promedios[i][1]\n",
    "\n",
    "train[\"bathrooms\"][train[\"bathrooms\"].isna()] = 0 #-- Aquelas filas de la columna baño que no pudieron ser completadas al ser \"nan\" las normalizo con 0.\n",
    "train.bathrooms = train.bathrooms.astype(int) #-- Para no tener decilamales, uso \"int\" para tener solo numeros enteros.\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#APLICADO AL DATASET DE TEST\n",
    "\n",
    "promedios_t = []  \n",
    "\n",
    "for i in range(len(test.property_type.value_counts().index)):  \n",
    "    tipo = test.property_type.value_counts().index[i],test.bathrooms [test.property_type == test.property_type.value_counts().index[i]].mean()\n",
    "    promedios_t.append(tipo)\n",
    "\n",
    "for i in range (len(promedios_t)): \n",
    "    test.bathrooms[(test.bathrooms.isna()) & (test.property_type == promedios_t[i][0])] = promedios_t[i][1]\n",
    "\n",
    "test[\"bathrooms\"][test[\"bathrooms\"].isna()] = 0 \n",
    "\n",
    "test.bathrooms = test.bathrooms.astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETANDO COLUMNA DE L3 PARA POSTERIOR COMPLETADO DE \"LAT\" Y \"LON\"\n",
    "\n",
    "localilades_na = train[\"l2\"][train.l3.isna()].drop_duplicates().values \n",
    "# Lista de \"l2\" que poseen \"l3\" con nulos, usando la moda de \"l3\" con el filtro de \"l2\",                                                                   \n",
    "# se puede conseguir un valor mas preciso para completar los registros de \"l3\" faltantes que sean a.                                                        \n",
    "for i in localilades_na:\n",
    "    if len(train[\"l3\"][train[\"l2\"] == i].mode()) > 0: #-- condicion por si la moda devuelve nulo y evitar que el loop de error.\n",
    "        train[\"l3\"][(train[\"l2\"] == i) & (train[\"l3\"].isna())] = train[\"l3\"][train[\"l2\"] == i].mode()[0]\n",
    "\n",
    "\n",
    "train[\"l3\"][train[\"l3\"].isna()] = \"sin dato\" #-- Gracias a este codigo solo quedarian 13 valores faltantes que se terminan completando y normalizando con el valor de \"sin dato\"\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#APLICADO AL DATASET DE TEST\n",
    "\n",
    "localilades_na_test = test[\"l2\"][test.l3.isna()].drop_duplicates().values                                                                    \n",
    "                                                                        \n",
    "for i in localilades_na:\n",
    "    if len(test[\"l3\"][test[\"l2\"] == i].mode()) > 0:\n",
    "        test[\"l3\"][(test[\"l2\"] == i) & (test[\"l3\"].isna())] = test[\"l3\"][test[\"l2\"] == i].mode()[0]\n",
    "    \n",
    "test[\"l3\"][test[\"l3\"].isna()] = \"sin dato\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETADO DE COLUMNAS \"LAT\" Y \"LON\"\n",
    "\n",
    "\n",
    "# Por ultimo vamos a completar las columnas \"lat\" y \"lon\" utilizando el promedio de los mismos registros que ahi en sus respectivas\n",
    "# columnas teniendo en cuenta que este promedio de los faltantes vengan con respaldo de \"l2\" y \"l3\" para asegurar una mejor presicion.\n",
    "\n",
    "\n",
    "lat_lon = train[[\"l2\",\"l3\"]][(train[\"lat\"].isna()) & (train[\"lon\"].isna())].drop_duplicates().values # Lista de \"l2\" y \"l3\" que estan en las filas donde no hay ni latitud ni longitud.\n",
    "\n",
    "for i in lat_lon:\n",
    "    print (i)\n",
    "    prom_lon = train[\"lon\"][(train[\"l2\"]== i[0]) & (train[\"l3\"]== i[1])].mean()\n",
    "    prom_lat = train[\"lat\"][(train[\"l2\"]== i[0]) & (train[\"l3\"]== i[1])].mean()\n",
    "    print (prom_lon,prom_lat)\n",
    "    train[\"lon\"][(train[\"lon\"].isna()) & (train[\"l2\"]== i[0]) & (train[\"l3\"]== i[1])] = prom_lon\n",
    "    train[\"lat\"][(train[\"lat\"].isna()) & (train[\"l2\"]== i[0]) & (train[\"l3\"]== i[1])] = prom_lat\n",
    "\n",
    "#-- Gracias a este codigo solo quedan 70 filas en longitud y latitud sin informacion. para mi modelo voy a utilizar el metodo de promedio en la columna de \"lon\" y \"lat\"\n",
    "# respectivamente, para asi completar los valores faltantes con esa aproximacion.\n",
    "\n",
    "train[\"lon\"][train[\"lon\"].isna()] = train[\"lon\"].mean()\n",
    "train[\"lat\"][train[\"lat\"].isna()] = train[\"lat\"].mean()\n",
    "\n",
    "train.drop(columns=[\"l2\",\"l3\"],inplace=True) \n",
    "#Al ser columnas con muchas variables categoricas las dropeo, de esa manera puedo ejecutar un modelo sencillo, con la mayoria de mis registros\n",
    "#completados y columnas con registros numericos.\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#APLICADO AL DATASET DE TEST\n",
    "\n",
    "lat_lon_test = test[[\"l2\",\"l3\"]][(test[\"lat\"].isna()) & (test[\"lon\"].isna())].drop_duplicates().values\n",
    "\n",
    "for i in lat_lon:\n",
    "    print (i)\n",
    "    prom_lon = test[\"lon\"][(test[\"l2\"]== i[0]) & (test[\"l3\"]== i[1])].mean()\n",
    "    prom_lat = test[\"lat\"][(test[\"l2\"]== i[0]) & (test[\"l3\"]== i[1])].mean()\n",
    "    print (prom_lon,prom_lat)\n",
    "    test[\"lon\"][(test[\"lon\"].isna()) & (test[\"l2\"]== i[0]) & (test[\"l3\"]== i[1])] = prom_lon\n",
    "    test[\"lat\"][(test[\"lat\"].isna()) & (test[\"l2\"]== i[0]) & (test[\"l3\"]== i[1])] = prom_lat\n",
    "\n",
    "test[\"lon\"][test[\"lon\"].isna()] = test[\"lon\"].mean()\n",
    "test[\"lat\"][test[\"lat\"].isna()] = test[\"lat\"].mean()\n",
    "\n",
    "test.drop(columns=[\"l2\",\"l3\"],inplace=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) ONEHOTENCODER para mi columna categorica, y la creacion de la columna TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizamos el metodo OneHotEncoder para poder modificar mi columna categorica \"property_type\" a una numerica.\n",
    "codificador = OneHotEncoder()\n",
    "cod_property_type = codificador.fit_transform(train[[\"property_type\"]])\n",
    "property_type = pd.DataFrame(cod_property_type.toarray(),columns=codificador.categories_)\n",
    "\n",
    "train.reset_index(inplace=True) #-- Reseteo el indice para de esa manera no tener erroes en la concatenacion.\n",
    "\n",
    "\n",
    "#Concateno las tablas train (tabla madre) con la tabla numerica de mi columna que era categorica \"property_type\"\n",
    "train = pd.concat([train,property_type],axis=\"columns\")\n",
    "train.drop(columns=\"property_type\",inplace=True) #-- Dropeo columna con valores categoricas que ya transforme con onehotencoder.\n",
    "train.drop(columns=\"index\",inplace=True) #-- Dropeo columna con indices que me genero pandas.\n",
    "\n",
    "\n",
    "\n",
    "#Creo mi variable objetivo con la primicia de que si el precio esta por encima del promedio de la columa \"price\" es caro (1) caso contrario es barato (0)\n",
    "train[\"target\"] = 0\n",
    "promedio = round(train[\"price\"].mean(),2)\n",
    "train[\"target\"] = train[\"price\"].apply(lambda x: 1 if x > promedio else 0)\n",
    "train.drop(columns=\"price\",inplace=True) #-- Dropeo columna de precio ya que para la evaluacion de mi modelo no es informacion util\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "#APLICADO AL DATASET DE TEST\n",
    "\n",
    "codificador_t = OneHotEncoder()\n",
    "cod_property_type_t = codificador_t.fit_transform(test[[\"property_type\"]])\n",
    "property_type_test = pd.DataFrame(cod_property_type_t.toarray(),columns=codificador_t.categories_)\n",
    "test.reset_index(inplace=True)\n",
    "test = pd.concat([test,property_type_test],axis=\"columns\")\n",
    "test.drop(columns=\"property_type\",inplace=True) \n",
    "test.drop(columns=\"index\",inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) importamos los metodos correspondientes para la creacion de mi modelo de categorizacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Decidi utilizar el modelo de regresion logisitica ya que no solo me dio buenos resultados luego de probarlo, sino que de \n",
    "#antemano pense en que si bien podia ser una solucion sencilla al promebla de la prediccion, encajaba bastante bien\n",
    "#con el dataset final que transforme, dejando pocas columnas pero la mayoria de registros en estas intactos y completos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[train.columns[:-1]].to_numpy() #-- Creo mi set de datos para entrenar (sin mi variable objetivo)\n",
    "Y = train[train.columns[-1]].to_numpy() #--  Creo mi set de entrenamiento sin mi variable objetivo\n",
    "\n",
    "\n",
    "X_Train,X_Test,Y_Train,Y_Test = train_test_split(X,Y,test_size=0.30,random_state=77)\n",
    "#Seleccionamos un 30% de datos para testear y el restante 70% para entrenar mi modelo\n",
    "\n",
    "modelo=LogisticRegression(solver='lbfgs', max_iter=10000).fit(X_Train,Y_Train) #-- Entrenamos al modelo.\n",
    "Y_predicho = modelo.predict(X_Test) #-- resultados de la prediccion de mi modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) evaluacion de mi modelo con la importacion de las metricas para la matriz de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34838662684827737"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(Y_Test,Y_predicho) # -- recall de 0.35\n",
    "metrics.accuracy_score(Y_Test,Y_predicho) # -- accuracy_ de 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "testeo = test.to_numpy()\n",
    "resultado = modelo.predict(testeo)# -- prediccion con dataset de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(resultado)\n",
    "df.rename(columns={0:\"pred\"},inplace=True)\n",
    "df.to_csv(\"nicolasgallu.csv\",index=False)\n",
    "\n",
    "#archivo  \"nicolasgallu\" subido como resultados de las predicciones de mi modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85c576d62e5a69baba7dcae6282c7bf6fba6f8d537c9cbb11ca984aece3c77a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
